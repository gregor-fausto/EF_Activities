---
title: "Chapter 6 - Hierarchical Bayes"
output: html_document
---

The objective of this activity is to explore basic hierarchical models.  We will focus on the most common class of hierarchical models, which are hierarchical linear models. These models are similar in structure to frequentist "mixed models", which are modelsthat include both hierarchical “random” effects and non-hierarchical “fixed” effects.  Everything that we apply below to linear models can also be applied to generalized linear models (e.g. logistic and poisson regression) and thus falls within the class of Bayesian hierarchical GLMs, which are analogous to frequentist GLMM (generalized linear mixed models), and for which all of our traditional non-hierarchical linear and GLM exist as a special case. Hierarchical approaches can also be applied to non-linear and process-based models to capture unexplained variability in any model parameters. In more advanced applications parameters can be described as varying according to some temporal or spatial autocorrelation, as opposed to the assumption of independence we assume in this simple case.

# Case Study: Mosquito population size

For this activity we will look at data on mosquito abundance.  The data file “Mosquito.csv” contains ten years worth of data for each of 5 replicate traps.   We will begin with the simplest possible model to explain this data and incrementally add complexity.

```{r}
dat <- read.csv("data/Mosquito.csv",header=TRUE,as.is = TRUE)
```

### Task 1: 

1.  Plot mosquito abundance as a function of time in a way that distinguishes the replicates and connects them through time (e.g. with lines, colors, or symbols)

```{r}
library(tidyverse)

ggplot(data = dat) +
  geom_line(aes(x=as.factor(as.character(time)), y = density, group = replicate))

ggplot(data = dat,aes(x=as.factor(as.character(time)), y = density, group = replicate, color=as.factor(replicate))) +
  geom_point() +
  geom_line()
```
2.	Write and run a JAGS model to fit the overall "global" mean `mu` and standard deviation `sigma`, reporting summary statistics for both. You can use the JAGS code you developed from activity 5 for this.

Define JAGS model:

```{r}
global_mean <- "
model {
  
  mu ~ dnorm(mu0,tau) # prior on global mean
  S ~ dgamma(0.1,0.1)
  
  for(i in 1:N){
    y[i] ~ dnorm(mu,S) # data model
  }
}
"
```

```{r}
library(rjags)

mean(dat$density);
sd(dat$density);

library(tidybayes)

data <- list(y = dat$density,
             N = nrow(dat),
             mu0 = 0,
             tau = 0.0001)

# initials
nchain = 3
inits <- list()
for(i in 1:nchain){
  inits[[i]] <- list(S = runif(1, 1/200,1/20))
}

j.globalMean <- jags.model(file = textConnection(global_mean),
                           data=data,
                           inits= inits,
                           n.adapt = 5000,
                           n.chains = 3)

norm.out <- coda.samples( model = j.globalMean,
                          variable.names = c("mu","S"),
                          n.iter = 10000)


```

```{r}
burnin = 2000
norm.burn <- window(norm.out, start=burnin)
effectiveSize(norm.burn)
plot(norm.burn)
summary(norm.burn)
```

3.	Add posterior CI and PI to the plot.

Begin by sampling from the posterior.

```{r}
norm.mat <- as.matrix(norm.out)
```


```{r}
## credible and prediction intervals
nsamp <- 5000
samp <- sample.int(nrow(norm.mat),nsamp)
xpred <- unique(dat$time)   					## sequence of x values we're going to
npred <- length(xpred)				##      make predictions for
ypred <- matrix(0.0,nrow=nsamp,ncol=npred)	## storage for predictive interval
ycred <- matrix(0.0,nrow=nsamp,ncol=npred)	## storage for credible interval
```

Next we'll set up a loop where we'll calculate the expected value of y at each x for each pair of regression parameters and then add additional random error from the data model.  When looping through the posterior MCMC we'll obviously want to account for any burn-in period and thinning 

```{r}
for(g in seq_len(nsamp)){
  post = norm.mat[samp[g],]
  ycred[g,] <- post["mu"]
  ypred[g,] <- rnorm(npred,ycred[g,],1/sqrt(post["S"]))
  #ycred[g,] <- theta["beta[1]"] + theta["beta[2]"]*xpred
  #ypred[g,] <- rnorm(npred,ycred[g,],1/sqrt(theta["prec"]))
}
```

Once we have the full matrix of predicted values we'll calculate the quantiles by column (ie for each x value) and then plot them vs. the data.  By selecting the 2.5% and 97.5% quantiles we are generating a 95% interval, because 95% of the calculated values fall in the middle and 2.5% fall in each of the upper and lower tails.  We could construct alternative interval estimates by just calculating different quantiles, for example the 5% and 95% quantiles would provide a 90% interval estimate. 

```{r}
ci <- apply(ycred,2,quantile,c(0.025,0.5,0.975))  ## credible interval and median
pi <- apply(ypred,2,quantile,c(0.025,0.975))		## prediction interval

ci <- data.frame(time = unique(dat$time),t(ci))
pi <- data.frame(time = unique(dat$time),t(pi))

ggplot() +
  geom_point(data = dat, aes(x=as.factor(as.character(time)), y = density, group = replicate)) +
  geom_line(data= ci, aes(x=as.factor(as.character(time)), y = X2.5.,group=1),linetype='dotted')+
  geom_line(data= ci, aes(x=as.factor(as.character(time)), y = X97.5.,group=1),linetype='dotted') +
  geom_line(data=pi, aes(x=as.factor(as.character(time)), y = X2.5.,group=1),linetype='dashed') +
  geom_line(data= pi, aes(x=as.factor(as.character(time)), y = X97.5.,group=1),linetype='dashed')

```

# Random time effect

From the graphs in Task 1 it should be apparent that there is systematic year-to-year variability that is unexplained by just a simple mean.  Since at this point we don't know the cause of this variability we can begin by adding a random effect for year.  

To add the random year effect:

1. Add the random year effect to the process model.
```
Ex[i] <- mu + alpha.t[time[i]]		## process model (varies with time but not rep)
```
Note that the version above is formatted slightly differently from the version covered in the lecture slides. In the lecture, the data were in a wide format, `x[t,b,i]`, where time, block, and individual were different dimensions in an array. Alternatively, one can format data in a long format, like we see in this file, with time and replicate as columns
```{r}
head(dat)
```
The variable `time` used in the code above is a vector of indices (length = nrow(dat)) matching a specific row of data to a specific `alpha.t`. Therefore, when building the `data` list that you pass into `jags.model` you'll want to add `time` and have that vector contain values in the range from 1 to 10 instead of 1995-2004. When working with long data, the easiest way to do this is to convert a column to a factor, then from a factor to an integrer
```{r}
dat$time.vector <- as.integer(as.factor(dat$time))
```

2. Update the data model to reference `Ex[t]` instead of `mu`

3. Add the random year effect parameter model (within a loop over time)
```
for(t in 1:nt){
  alpha.t[t] ~ dnorm(0,tau.t)		## random year effect
}
```

4. Add a prior on `tau.t`, the year-to-year variability

5. When sampling from your posteriors, make sure to track all of your unknown parameters: 
 + `mu` - global mean
 + `sigma` - residual error (inverse of $tau$)
 + `alpha_t` - random year effect
 + `tau_t` - year-to-year precision

### Task 2

4.  Fit the random-time model and turn in a plot like in Task 1 with the posterior CI and PI plotted against the data.

Update the JAGS model with the notes from the section above. As noted in the notes above, the notation here is different than from the powerpoint.

```{r}
partial_pooling <- "
model {
  
  mu ~ dnorm(0,0.001)   ## prior
  tau ~ dgamma(0.001,0.001) ## prior
  
  tau.t ~ dgamma(0.001, 0.001) ## hyperprior
  for(t in 1:nt){
  alpha.t[t] ~ dnorm(0,tau.t)		## random year effect
  }

  for(i in 1:N){
    Ex[i] <- mu + alpha.t[time[i]]		## process model (varies with time but not rep)
    y[i] ~ dnorm(Ex[i],tau) # data model
  }
}
"
```

```{r}
library(rjags)
library(tidybayes)

data <- list(y = dat$density,
             N = nrow(dat),
             time = dat$time.vector,
             nt = max(dat$time.vector))

# initials
nchain = 3
inits <- list()
for(i in 1:nchain){
  inits[[i]] <- list(tau = runif(1, 1/200,1/20))
}

j.partialPooling <- jags.model(file = textConnection(partial_pooling),
                           data=data,
                           inits= inits,
                           n.adapt = 5000,
                           n.chains = 3)

out <- coda.samples( model = j.partialPooling,
                          variable.names = c("mu","alpha.t","tau","tau.t"),
                          n.iter = 10000)
```

```{r}
burnin = 2000
out.burn <- window(out, start=burnin)
effectiveSize(out.burn)
plot(out.burn)
summary(out.burn)

library(runjags)
MCMCvis::MCMCsummary(out.burn)
```

Begin by sampling from the posterior.

```{r}
jags.out <- out

jags.mat <- as.matrix(jags.out)
sel.a <- grep("alpha",colnames(jags.mat))
plot(jags.out[,sel.a])
summary(jags.out[,sel.a])
alpha <- jags.mat[,sel.a]
apply(alpha,2,mean)

```


```{r}
## credible and prediction intervals
nsamp <- 5000
samp <- sample.int(nrow(jags.mat),nsamp)
xpred <- unique(dat$time)   					## sequence of x values we're going to
npred <- length(xpred)				##      make predictions for
ypred <- matrix(0.0,nrow=nsamp,ncol=npred)	## storage for predictive interval
ycred <- matrix(0.0,nrow=nsamp,ncol=npred)	## storage for credible interval
```

Next we'll set up a loop where we'll calculate the expected value of y at each x for each pair of regression parameters and then add additional random error from the data model.  When looping through the posterior MCMC we'll obviously want to account for any burn-in period and thinning 

```{r}
for(g in seq_len(nsamp)){
  post = jags.mat[samp[g],]
  ycred[g,] <- post["mu"] + post[grep("alpha.t" , names(post))]
  ypred[g,] <- rnorm(npred,ycred[g,],1/sqrt(post["tau"]))
}
```

Once we have the full matrix of predicted values we'll calculate the quantiles by column (ie for each x value) and then plot them vs. the data.  By selecting the 2.5% and 97.5% quantiles we are generating a 95% interval, because 95% of the calculated values fall in the middle and 2.5% fall in each of the upper and lower tails.  We could construct alternative interval estimates by just calculating different quantiles, for example the 5% and 95% quantiles would provide a 90% interval estimate. 

```{r}
ci <- apply(ycred,2,quantile,c(0.025,0.5,0.975))  ## credible interval and median
pi <- apply(ypred,2,quantile,c(0.025,0.975))		## prediction interval

ci <- data.frame(time = unique(dat$time),t(ci))
pi <- data.frame(time = unique(dat$time),t(pi))

ggplot() +
  geom_point(data = dat, aes(x=as.factor(as.character(time)), y = density, group = replicate)) +
  geom_line(data= ci, aes(x=as.factor(as.character(time)), y = X2.5.,group=1),linetype='dotted')+
  geom_line(data= ci, aes(x=as.factor(as.character(time)), y = X50.,group=1),linetype='solid')+
    geom_line(data= ci, aes(x=as.factor(as.character(time)), y = X97.5.,group=1),linetype='dotted') +
  geom_line(data=pi, aes(x=as.factor(as.character(time)), y = X2.5.,group=1),linetype='dashed') +
  geom_line(data= pi, aes(x=as.factor(as.character(time)), y = X97.5.,group=1),linetype='dashed')

```

Hint: once you convert the JAGS coda object to a matrix, you can use `grep` to figure out which columns contain alphas: 
```
jags.mat <- as.matrix(jags.out)
sel.a <- grep("alpha",colnames(jags.mat))
plot(jags.out[,sel.a])
summary(jags.out[,sel.a])
alpha <- jags.mat[,sel.a]
apply(alpha,2,mean)
```
5.	Looking at the posterior estimates for tau and sigma, how much of the variance in the mosquito densities is explained by the year effects? 

```{r}
library(runjags)
MCMCvis::MCMCsummary(out.burn)

prec <- MCMCvis::MCMCchains(out.burn,params=c("tau","tau.t"))
vars <-1/apply(prec,2,quantile,probs=c(0.025,.5,.975))
vars
```
If I've done this correctly, the year effects explain roughly equal amounts of variation in the dataset. But I'm not sure how to get from here to interpreting how much of the total variance is explained by the year effects. 

6. Describe how you would modify your code to add a random `replicate` effect.

To add a random `replicate` effect, I would add another term to the linear predictor. The JAGS code for the model would look like this:

```{r}
partial_pooling2 <- "
model {
  
  mu ~ dnorm(0,0.001)   ## prior
  tau ~ dgamma(0.001,0.001) ## prior
  
  tau.t ~ dgamma(0.001, 0.001) ## hyperprior
  for(t in 1:nt){
  alpha.t[t] ~ dnorm(0,tau.t)		## random year effect
  }
  
  tau.b ~ dgamma(0.001, 0.001) ## hyperprior
  for(b in 1:nb){
  alpha.b[b] ~ dnorm(0,tau.b)		## random year effect
  }

  for(i in 1:N){
    Ex[i] <- mu + alpha.t[time[i]] + alpha.b[replicate[i]]		## process model (varies with time but not rep)
    y[i] ~ dnorm(Ex[i],tau) # data model
  }
}
"
```

# Combining Linear and Random Effects

You are discussing your research with a colleague and mention that your random effects model showed that one year, 2002, had notably lower mosquito abundance.  He suggests that the driver may be exogenous and sends you a data file, met.csv, that contains the mean annual temperature (°C), precipitation (mm/year), and relative humidity (%) for 1995-2009 years.
 
### Task 3:

6.  As an exploratory analysis of this hypothesis, plot the posterior mean of your random year effect (alpha_t) versus each of the three met variables.  Which variable(s) are worth exploring further?
7.	Convert the random effects model to a hierarchical linear model by converting the mean, mu, to a linear model, `beta0 + beta1*y[t]` where y is the meteorological covariate you want to include, while keeping the random year effect.
8.	Fit your hierarchical linear model and plot the model CI and PI vs the data
9.	Create a summary table that provides the posterior parameter means and CI for all 3 models and their DIC scores.
10.	Extra Credit: Use the best fitting model to predict the next 5 years (2005-2009) of mosquito abundance including an uncertainty estimate (predictive interval). Turn in a graph of your prediction.

## Beyond the Basics

In this execise we fit a hierarchical linear model to account for variability in the mean. However, this approach can be used more generally to account for variability in any model parameters -- for example we could write down a simple logistic population model where `r` and `K` themselves are functions of multiple covariates (fixed effects) but also have unexplained variability across multiple scales (multiple random effects). These random effects don't just have to apply to different years, they could also apply to different locations (subpopulations, plots, watersheds, etc) that could have multiple heirchical levels (e.g. plots with sites). For some analyses it might make sense to have random effects on individuals, or even parts of individuals (e.g. leaves on a tree), so long as multiple measurements are made on the same observational unit. 

The other thing we assumed in this example was that each random effect was drawn independently from the same distribution

```
for(t in 1:NT){
 alpha_t[t] ~ dnorm(0,tau_t)  ## random year effect
}
```

But it is conceptually straightforward to generalize the current assumption to one where random effects might be correlated in space, time, phylogeny, or in some other network (rivers, social, etc):

```
alpha_t ~ dmnorm(0,TAU_T)
```

where `TAU_T` is now a covariance matrix and `alpha_t` is the vector of all the alphas drawn from the multivariate normal `dmnorm`. The construction of `TAU_T` is typically broken down into two parts, one describing the overall variance and the other descibing how the correlation between any two alphas changes as a function of the distance between them (in time, space, network, etc). For example, since `alpha_t` is a year effect we might model it using a standard autoregressive (AR) timeseries approach

```
  TAU_T <- inverse((1/tau_t)/(1-rho^2)*rho^H) ## AR(1) covariance matrix
  tau_t ~ dgamma(t1,t2) ## prior on overall precision
  rho   ~ dunif(-1,1)   ## prior on autocorrelation parameter
```
where `H` is a matrix describing the pairwise distance (in time) between the `alpha_t`s. Similar covariance formulations exist for other forms of autocorrelation, and the approach is quite general so long as the correlation can be expressed as a function of some sort of distance or adjacency.  

Finally, when moving beyond the basics I strongly recommend that you start simple, add complexity incrementally, and assess/test your assumptions before adding more. From personal experience I can tell you that I once spent months getting a complex space-time stage-structured model working only to discover that there was no spatial autocorrelation in the residuals and the model needed to be simplified considerably. Check for autocorrelation before assuming it. Likewise, as we did in Task 3, evaluate random effects to see if there is variability that needs explaining before developing complex fixed-effect models to explain that variability.